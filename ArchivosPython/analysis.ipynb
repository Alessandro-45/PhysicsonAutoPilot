{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b971933d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install atlasopenmagic\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import uproot\n",
    "import awkward as ak\n",
    "import vector\n",
    "import atlasopenmagic as atom\n",
    "\n",
    "# --- Configuración del Análisis ---\n",
    "atom.set_release('2025e-13tev-beta')\n",
    "fraction = 0.2 # Usar una fracción de los datos para acelerar la ejecución\n",
    "skim = \"exactly4lep\"\n",
    "GeV = 1.0\n",
    "\n",
    "# --- Definiciones de Samples y Funciones ---\n",
    "defs = {\n",
    "    r'Background $Z,t\\bar{t},t\\bar{t}+V,VVV$':{'dids': [410470,410155,410218,410219,412043,364243,364242,364246,364248,700320,700321,700322,700323,700324,700325], 'color': \"#6b59d3\" },\n",
    "    r'Background $ZZ^{*}$': {'dids': [700600],'color': \"#ff0000\" },\n",
    "    r'Signal ($m_H$ = 125 GeV)':  {'dids': [345060, 346228, 346310, 346311, 346312, 346340, 346341, 346342],'color': \"#00cdff\" },\n",
    "}\n",
    "\n",
    "def cut_lep_type(lep_type):\n",
    "    sum_lep_type = ak.sum(lep_type, axis=1)\n",
    "    return (sum_lep_type != 44) & (sum_lep_type != 48) & (sum_lep_type != 52)\n",
    "\n",
    "def cut_lep_charge(lep_charge):\n",
    "    return ak.sum(lep_charge, axis=1) != 0\n",
    "\n",
    "def calc_mass(lep_pt, lep_eta, lep_phi, lep_e):\n",
    "    p4 = vector.zip({\"pt\": lep_pt, \"eta\": lep_eta, \"phi\": lep_phi, \"E\": lep_e})\n",
    "    return (p4[:, 0] + p4[:, 1] + p4[:, 2] + p4[:, 3]).m\n",
    "\n",
    "def cut_trig_match(lep_trigmatch):\n",
    "    return ak.sum(lep_trigmatch, axis=1) >= 1\n",
    "\n",
    "def cut_trig(trigE, trigM):\n",
    "    return trigE | trigM\n",
    "\n",
    "def ID_iso_cut(IDel, IDmu, isoel, isomu, pid):\n",
    "    return (ak.sum(((pid == 13) & IDmu & isomu) | ((pid == 11) & IDel & isoel), axis=1) == 4)\n",
    "\n",
    "def calc_weight(lumi, weight_variables, events):\n",
    "    total_weight = (lumi * 1000) / events[\"sum_of_weights\"]\n",
    "    for variable in weight_variables:\n",
    "        total_weight = total_weight * abs(events[variable])\n",
    "    return total_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c6c78d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# --- Configuración del Histograma ---\n",
    "m_min = 80      # GeV\n",
    "m_max = 170     # GeV\n",
    "step_size = 2   # GeV\n",
    "bin_edges = np.arange(start=m_min, stop=m_max, step=step_size)\n",
    "\n",
    "# ## 6. Bucle Principal de Análisis por Período (con filtrado por RunYear)\n",
    "#\n",
    "# Estrategia: Cargar todos los datos y simulaciones una vez, y luego filtrar por\n",
    "# la variable 'RunYear' para generar los JSONs para cada período.\n",
    "\n",
    "# --- 1. Carga y procesamiento inicial de todos los datos ---\n",
    "\n",
    "# Variables a leer, AHORA SIN 'RunYear' directametne del tree para los datos\n",
    "variables = ['lep_pt', 'lep_eta', 'lep_phi', 'lep_e', 'lep_charge', 'lep_type', 'lep_isTrigMatched', 'lep_isLooseID', 'lep_isMediumID', 'lep_isTightID', 'lep_isLooseIso', 'lep_isTightIso', 'trigE', 'trigM']\n",
    "weight_variables = ['mcWeight', 'scaleFactor_PILEUP', 'scaleFactor_ELE', 'scaleFactor_MUON', 'scaleFactor_LepTRIGGER']\n",
    "\n",
    "# Usamos 'defs' y añadimos 'Data' con el atajo 'data' que agrupa todos los años\n",
    "defs_total = { 'Data': {'dids': ['data']} }\n",
    "for key, value in defs.items():\n",
    "    defs_total[key] = value\n",
    "\n",
    "print(\"Cargando y procesando todos los samples (esto puede tardar)...\")\n",
    "samples_total = atom.build_dataset(defs_total, skim=skim, protocol='https')\n",
    "all_data_unfiltered = {}\n",
    "\n",
    "for s in samples_total:\n",
    "    print('Procesando sample: ' + s)\n",
    "    frames = []\n",
    "    for val in samples_total[s]['list']:\n",
    "        start = time.time()\n",
    "        try:\n",
    "            tree = uproot.open(val + \":analysis\")\n",
    "            print(f\"\\tArchivo: {val.split('/')[-1]}\")\n",
    "\n",
    "            # Leemos todas las variables necesarias. 'RunYear' se añadirá manualmente para Data.\n",
    "            iter_vars = variables + weight_variables + [\"sum_of_weights\"] if 'data' not in s.lower() else variables + [\"sum_of_weights\"]\n",
    "\n",
    "            # Iterate and process data\n",
    "            for data in tree.iterate(iter_vars, library=\"ak\", entry_stop=tree.num_entries * fraction):\n",
    "                # Para los datos, extraemos el RunYear del nombre del archivo\n",
    "                if 'data' in s.lower():\n",
    "                    match = re.search(r'data(\\d{2})', val)\n",
    "                    if match:\n",
    "                        year_suffix = int(match.group(1))\n",
    "                        run_year = 2000 + year_suffix # Asumiendo años 20xx\n",
    "                        # Añadimos RunYear al array de awkward\n",
    "                        data['RunYear'] = ak.Array([run_year] * len(data))\n",
    "                    else:\n",
    "                        print(f\"Warning: No se pudo extraer RunYear del nombre del archivo: {val}. Saltando este chunk.\")\n",
    "                        continue # Saltar este chunk si no se puede determinar el año\n",
    "\n",
    "                # Aplicamos todos los cortes comunes\n",
    "                data = data[cut_trig(data.trigE, data.trigM)]\n",
    "                data = data[cut_trig_match(data.lep_isTrigMatched)]\n",
    "                data = data[ak.all(data.lep_pt > 10, axis=1)]\n",
    "                data = data[data.lep_pt[:,0] > 20]\n",
    "                data = data[data.lep_pt[:,1] > 15]\n",
    "                data = data[ID_iso_cut(data.lep_isLooseID, data.lep_isMediumID, data.lep_isLooseIso, data.lep_isLooseIso, data.lep_type)]\n",
    "                data = data[~cut_lep_type(data.lep_type)]\n",
    "                data = data[~cut_lep_charge(data.lep_charge)]\n",
    "\n",
    "                if len(data) > 0:\n",
    "                    frames.append(data)\n",
    "\n",
    "            elapsed = time.time() - start\n",
    "            print(f\"\\t...hecho en {round(elapsed,1)}s\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\tError procesando archivo {val}: {e}. Saltando este archivo.\")\n",
    "            continue\n",
    "\n",
    "    if frames:\n",
    "        all_data_unfiltered[s] = ak.concatenate(frames)\n",
    "\n",
    "print(\"\\n--- ¡Carga completa! Ahora generando JSON por período... ---\")\n",
    "\n",
    "# --- 2. Bucle por período para filtrar, calcular y guardar ---\n",
    "periodos = {\n",
    "    '2015-2016': {'years': [2015, 2016], 'lumi': 36.2},\n",
    "    '2017': {'years': [2017], 'lumi': 44.3},\n",
    "    '2018': {'years': [2018], 'lumi': 59.5},\n",
    "    'Total': {'years': [2015, 2016, 2017, 2018], 'lumi': 139.0}\n",
    "}\n",
    "\n",
    "for nombre_periodo, info_periodo in periodos.items():\n",
    "    print(f\"\\n{'='*60}\\n========== FILTRANDO PARA PERÍODO: {nombre_periodo} ==========\\n{'='*60}\")\n",
    "    lumi = info_periodo['lumi']\n",
    "\n",
    "    all_data_periodo = {}\n",
    "    for s, data in all_data_unfiltered.items():\n",
    "        if not len(data): continue\n",
    "\n",
    "        # Filtramos los datos reales por año. La simulación (MC) se usa completa.\n",
    "        if s == 'Data':\n",
    "            # Asegurarse de que 'RunYear' existe en los datos, que ahora es inyectado\n",
    "            if 'RunYear' in data.fields:\n",
    "                mask = (data.RunYear == info_periodo['years'][0])\n",
    "                for year in info_periodo['years'][1:]:\n",
    "                    mask = mask | (data.RunYear == year)\n",
    "                filtered_data = data[mask]\n",
    "            else:\n",
    "                print(f\"Warning: 'RunYear' no encontrado en el sample de Data para el período {nombre_periodo}. Saltando.\")\n",
    "                filtered_data = ak.Array([]) # Vaciar si no hay RunYear\n",
    "        else:\n",
    "            filtered_data = data # Usamos toda la simulación\n",
    "\n",
    "        # Calculamos masa y pesos DESPUÉS de filtrar\n",
    "        if len(filtered_data) > 0:\n",
    "            filtered_data['mass'] = calc_mass(filtered_data.lep_pt, filtered_data.lep_eta, filtered_data.lep_phi, filtered_data.lep_e)\n",
    "            if 'data' not in s.lower():\n",
    "                filtered_data['totalWeight'] = calc_weight(lumi, weight_variables, filtered_data)\n",
    "\n",
    "        all_data_periodo[s] = filtered_data\n",
    "\n",
    "    # --- 3. PREPARACIÓN PARA EXPORTACIÓN A JSON ---\n",
    "    # (El resto del código es igual, pero usa 'all_data_periodo')\n",
    "    if 'Data' in all_data_periodo and 'mass' in all_data_periodo['Data'].fields:\n",
    "        data_x, _ = np.histogram(ak.to_numpy(all_data_periodo['Data']['mass']), bins=bin_edges)\n",
    "    else:\n",
    "        data_x = np.zeros(len(bin_edges) - 1)\n",
    "    data_x_errors = np.sqrt(data_x)\n",
    "\n",
    "    signal_key = r'Signal ($m_H$ = 125 GeV)'\n",
    "    if signal_key in all_data_periodo and 'mass' in all_data_periodo[signal_key].fields:\n",
    "        signal_x = ak.to_numpy(all_data_periodo[signal_key]['mass'])\n",
    "        signal_weights = ak.to_numpy(all_data_periodo[signal_key].totalWeight)\n",
    "    else:\n",
    "        signal_x, signal_weights = [], []\n",
    "    signal_color = samples_total[signal_key]['color']\n",
    "    signal_counts, _ = np.histogram(signal_x, bins=bin_edges, weights=signal_weights)\n",
    "\n",
    "    background_hists = []\n",
    "    for s in samples_total:\n",
    "        if s not in ['Data', signal_key]:\n",
    "            if s in all_data_periodo and 'mass' in all_data_periodo[s].fields:\n",
    "                mc_mass = ak.to_numpy(all_data_periodo[s]['mass'])\n",
    "                mc_weights = ak.to_numpy(all_data_periodo[s].totalWeight)\n",
    "                mc_counts, _ = np.histogram(mc_mass, bins=bin_edges, weights=mc_weights)\n",
    "            else:\n",
    "                mc_counts = np.zeros(len(bin_edges) - 1)\n",
    "            background_hists.append({\n",
    "                \"label\": s,\n",
    "                \"counts\": mc_counts.tolist(),\n",
    "                \"color\": samples_total[s]['color']\n",
    "            })\n",
    "\n",
    "    # --- 4. CREAR Y GUARDAR EL ARCHIVO JSON ---\n",
    "    final_plot_data = {\n",
    "        \"period\": nombre_periodo,\n",
    "        \"lumi\": lumi,\n",
    "        \"data\": {\"counts\": data_x.tolist(), \"errors\": data_x_errors.tolist(), \"label\": \"Data\"},\n",
    "        \"signal\": {\"counts\": signal_counts.tolist(), \"label\": signal_key, \"color\": signal_color},\n",
    "        \"backgrounds\": background_hists,\n",
    "        \"bins\": bin_edges.tolist(),\n",
    "        \"x_axis_label\": \"Masa Invariante 4l (GeV)\",\n",
    "        \"y_axis_label\": f\"Eventos / {step_size} GeV\"\n",
    "    }\n",
    "\n",
    "    output_filename = f\"datos_analisis_{nombre_periodo.replace('-', '_')}.json\"\n",
    "    with open(output_filename, 'w') as f:\n",
    "        json.dump(final_plot_data, f, indent=4)\n",
    "\n",
    "    print(f\"\\n--- ¡Éxito! Datos para el período {nombre_periodo} guardados en {output_filename} ---\\n\")\n",
    "\n",
    "print(\"¡TODOS LOS PERÍODOS HAN SIDO PROCESADOS!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
